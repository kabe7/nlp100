{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "datapath = \"data/neko.txt.mecazb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. 形態素解析結果の読み込み\n",
    "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
    "\n",
    "### memo\n",
    "フォーマット: `表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_keys\n",
    "surface, base, pos, pos1 = \"表層形\", \"基本形\", \"品詞\", \"品詞細分類\"\n",
    "prg = re.compile(r\"(?P<sur>.+?)\\t(?P<pos>[^,]+),(?P<pos1>[^,]+),([^,]+,){4}(?P<base>[^,]+).*\")\n",
    "\n",
    "# generator\n",
    "def _neko_morph_():\n",
    "    with open(datapath) as f:\n",
    "        for line in f:\n",
    "            res = prg.match(line)\n",
    "            if res:\n",
    "                yield {surface: res.group(\"sur\"), base: res.group(\"base\"), pos: res.group(\"pos\"), pos1: res.group(\"pos1\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31. 動詞\n",
    "動詞の表層形をすべて抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for morph in neko_morph:\n",
    "#    if morph[base] == \"動詞\":\n",
    "#        print(morph[surface])\n",
    "\n",
    "neko_morph = _neko_morph_()\n",
    "verbs = [morph[surface] for morph in neko_morph if morph[pos] == \"動詞\"]\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32. 動詞の原形\n",
    "動詞の原形をすべて抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neko_morph = _neko_morph_()\n",
    "original_verbs = [morph[base] for morph in neko_morph if morph[pos] == \"動詞\"]\n",
    "print(original_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33. サ変名詞\n",
    "サ変接続の名詞をすべて抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neko_morph = _neko_morph_()\n",
    "sahen_noun = [morph[surface] for morph in neko_morph if morph[pos1] == \"サ変接続\"]\n",
    "print(sahen_noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34. 「AのB」\n",
    "2つの名詞が「の」で連結されている名詞句を抽出せよ．\n",
    "\n",
    "### MEMO\n",
    "名詞->連体化の「の」->名詞　を探す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_keys\n",
    "surface, base, pos, pos1 = \"表層形\", \"基本形\", \"品詞\", \"品詞細分類\"\n",
    "prg = re.compile(r\"(?P<sur>.+?)\\t(?P<pos>[^,]+),(?P<pos1>[^,]+),([^,]+,){4}(?P<base>[^,]+).*\")\n",
    "\n",
    "# search\n",
    "compressed = \"\"\n",
    "morph_list = []\n",
    "with open(datapath) as f:\n",
    "    for line in f:\n",
    "        res = prg.match(line)\n",
    "        if res:\n",
    "            morph_list.append(res.group(\"sur\"))\n",
    "            if res.group(\"pos\") == \"名詞\":\n",
    "                compressed += \"n\"\n",
    "            elif res.group(\"pos1\") == \"連体化\":\n",
    "                compressed += \"t\"\n",
    "            else:\n",
    "                compressed += \"x\"\n",
    "    \n",
    "    for match in re.finditer(r\"ntn\",compressed):\n",
    "        start, end = match.start(), match.end()\n",
    "        print(''.join(morph_list[start:end]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35. 名詞の連接\n",
    "名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_keys\n",
    "surface, base, pos, pos1 = \"表層形\", \"基本形\", \"品詞\", \"品詞細分類\"\n",
    "prg = re.compile(r\"(?P<sur>.+?)\\t(?P<pos>[^,]+),(?P<pos1>[^,]+),([^,]+,){4}(?P<base>[^,]+).*\")\n",
    "\n",
    "# search\n",
    "compressed = \"\"\n",
    "morph_list = []\n",
    "with open(datapath) as f:\n",
    "    for line in f:\n",
    "        res = prg.match(line)\n",
    "        if res:\n",
    "            morph_list.append(res.group(\"sur\"))\n",
    "            if res.group(\"pos\") == \"名詞\":\n",
    "                compressed += \"n\"\n",
    "            elif res.group(\"pos1\") == \"連体化\":\n",
    "                compressed += \"t\"\n",
    "            else:\n",
    "                compressed += \"x\"\n",
    "    \n",
    "    longest, longest_phrase = 0, \"\"\n",
    "    for match in re.finditer(r\"nn+\",compressed):\n",
    "        start, end = match.start(), match.end()\n",
    "        phrase = '_'.join(morph_list[start:end])\n",
    "        print(phrase)\n",
    "        if longest <= end-start:\n",
    "            longest = end-start\n",
    "            longest_phrase = phrase\n",
    "    \n",
    "    print(\"longest: \", longest_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36. 単語の出現頻度\n",
    "文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．\n",
    "\n",
    "### MEMO\n",
    "UNIX:  \n",
    "`cut -f 1 data/neko.txt.mecab | sort | uniq -c | sort -r -k 1 > word_rank`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "with open(datapath,'r') as f:\n",
    "    l = []\n",
    "    for line in f:\n",
    "        elms = line.split()\n",
    "        l.append(elms[0])\n",
    "    \n",
    "    c = collections.Counter(l)\n",
    "    \n",
    "print('\\n'.join([' '.join([elm, str(n)]) for elm, n in c.most_common() if elm != \"EOS\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 37. 頻度上位10語\n",
    "出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(datapath,'r') as f:\n",
    "    l = []\n",
    "    for line in f:\n",
    "        elms = line.split()\n",
    "        l.append(elms[0])\n",
    "    \n",
    "    c = collections.Counter(l)\n",
    "    c.pop(\"EOS\")\n",
    "    \n",
    "xy = c.most_common(10)\n",
    "plt.bar(range(10), list(map(lambda t: t[1], xy)), tick_label=list(map(lambda t: t[0], xy)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 38. ヒストグラム\n",
    "単語の出現頻度のヒストグラム（横軸に出現頻度，縦軸に出現頻度をとる単語の種類数を棒グラフで表したもの）を描け．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(datapath,'r') as f:\n",
    "    l = []\n",
    "    for line in f:\n",
    "        elms = line.split()\n",
    "        l.append(elms[0])\n",
    "    \n",
    "    c = collections.Counter(l)\n",
    "    c.pop(\"EOS\")\n",
    "    \n",
    "counts = c.values()\n",
    "plt.hist(counts, bins = np.logspace(0,4,12))\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.title(\"histogram\")\n",
    "plt.xlabel(\"frequency(log)\")\n",
    "plt.ylabel(\"number of words(log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39. Zipfの法則\n",
    "単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(datapath,'r') as f:\n",
    "    l = []\n",
    "    for line in f:\n",
    "        elms = line.split()\n",
    "        l.append(elms[0])\n",
    "    \n",
    "    c = collections.Counter(l)\n",
    "    c.pop(\"EOS\")\n",
    "    \n",
    "counts = c.values()\n",
    "print()\n",
    "plt.plot(range(len(counts)), sorted(counts)[::-1])\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.title(\"Zipf's law\")\n",
    "plt.xlabel(\"ranking(log)\")\n",
    "plt.ylabel(\"number of occurances(log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
