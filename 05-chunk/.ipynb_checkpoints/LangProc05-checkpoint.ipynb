{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "datapath = \"data/neko.txt.cabocha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．\n",
    "\n",
    "### 準備\n",
    " ```\n",
    " brew install cabocha\n",
    " cabocha -f1 neko.txt > neko.txt.cabocha\n",
    " ```\n",
    " \n",
    "### memo\n",
    "クラスに`__repr__`を定義するとprint等で呼ばれた時の表示方法を指定できる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Morph object at 0x1a1bdb1a90>, <__main__.Morph object at 0x1a1bdb1ac8>, <__main__.Morph object at 0x1a1bdb16d8>, <__main__.Morph object at 0x1a1bdb1b00>, <__main__.Morph object at 0x1a1bdb1b38>]\n"
     ]
    }
   ],
   "source": [
    "prg_morph = re.compile(r\"(?P<sur>.+?)\\t(?P<pos>[^,]+),(?P<pos1>[^,]+),([^,]+,){4}(?P<base>[^,]+).*\")\n",
    "\n",
    "def gen_sentence():\n",
    "    morph_list = []\n",
    "    with open(datapath) as f:\n",
    "        for line in f:\n",
    "            res_m = prg_morph.match(line)\n",
    "            if res_m:\n",
    "                morph_list.append(Morph(res_m.group(\"sur\"), res_m.group(\"base\"), res_m.group(\"pos\"), res_m.group(\"pos1\")))\n",
    "            elif line == \"EOS\\n\":\n",
    "                yield morph_list\n",
    "                morph_list = []\n",
    "\n",
    "for i, sentence in enumerate(gen_sentence()):\n",
    "    if i== 3:\n",
    "        print(sentence)\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
    "\n",
    "### memo\n",
    "文節、形態素、文末の判定は正規表現でやる\n",
    "\n",
    "1. 文節\n",
    "\n",
    "    - 形態素は以降の行に書いてあるので後でappendするための受け皿をつくっておく。\n",
    "    - 係り元を覚えておくための辞書を作る。日本語は係り先の方が文の後ろにあるから先読みせず1行ずつ処理可能\n",
    "    - EOSが2行以上連なってるところが1文にカウントされてしまっている\n",
    "    \n",
    "2. 形態素\n",
    "\n",
    "    4章を丸々流用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, dst, srcs):\n",
    "        self.morphs = []\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"形態素をくっつけたものを表示\"\n",
    "        return ''.join(map(str, self.morphs))\n",
    "    \n",
    "    def append(self, morph):\n",
    "        \"文節に形態素を追加\"\n",
    "        self.morphs.append(morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 8\n",
      "この \t-> 書生というのは\n",
      "書生というのは \t-> 話である。\n",
      "時々 \t-> 捕えて\n",
      "我々を \t-> 捕えて\n",
      "捕えて \t-> 煮て\n",
      "煮て \t-> 食うという\n",
      "食うという \t-> 話である。\n",
      "話である。 \t-> None\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "prg_morph = re.compile(r\"(?P<sur>.+?)\\t(?P<pos>[^,]+),(?P<pos1>[^,]+),([^,]+,){4}(?P<base>[^,]+).*\")\n",
    "prg_chunk = re.compile(r\"\\* (?P<num>\\d+) (?P<dst>-?\\d+)D \\d+/\\d+ .*\")\n",
    "\n",
    "def gen_sentence():\n",
    "    chunk = Chunk(-1, []) # 名前確保用\n",
    "    sentence, chunk_srcs = [], defaultdict(list)\n",
    "    with open(datapath) as f:\n",
    "        for line in f:\n",
    "            res_m = prg_morph.match(line)\n",
    "            res_c = prg_chunk.match(line)\n",
    "            if res_c:\n",
    "                me, dst = int(res_c.group(\"num\")), int(res_c.group(\"dst\"))\n",
    "                chunk_srcs[dst].append(me)\n",
    "                chunk = Chunk(dst, chunk_srcs[me])\n",
    "                sentence.append(chunk)\n",
    "            elif res_m:\n",
    "                chunk.append(Morph(res_m.group(\"sur\"), res_m.group(\"base\"), res_m.group(\"pos\"), res_m.group(\"pos1\")))\n",
    "            else: # End of sentence\n",
    "                yield sentence\n",
    "                sentence, chunk_srcs = [], defaultdict(list)\n",
    "\n",
    "def dependency_parsing(n):\n",
    "    counter = 0\n",
    "    for sentence in gen_sentence():\n",
    "        if sentence:\n",
    "            counter += 1\n",
    "            if counter == n:\n",
    "                print(\"sentence\",counter)\n",
    "                for chunk in sentence:\n",
    "                    print(chunk, \"\\t->\", sentence[chunk.dst] if chunk.dst != -1 else None)\n",
    "\n",
    "dependency_parsing(8)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/43.txt\", \"w\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            if (\"名詞\" in list(map(lambda m: m.pos, chunk.morphs))) and \"動詞\" in list(map(lambda m: m.pos, sentence[chunk.dst].morphs)):\n",
    "                line = '\\t'.join(map(str, [chunk, sentence[chunk.dst]]))\n",
    "                f.write(line + \"\\n\") if chunk.dst != -1 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/45_CasePattern.txt\", \"w\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            morph_verbs = [morph.base for morph in chunk.morphs if morph.pos == \"動詞\"]\n",
    "            if morph_verbs:\n",
    "                predicate, *rest = morph_verbs\n",
    "                particles = [morph.base for morph in chunk.morphs if morph.pos == \"助詞\"]\n",
    "                if particles:\n",
    "                    f.write(''.join([predicate, '\\t', ' '.join(particles), '\\n']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
