{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "datapath = \"data/neko.txt.cabocha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．\n",
    "\n",
    "### 準備\n",
    " ```\n",
    " brew install cabocha\n",
    " cabocha -f1 neko.txt > neko.txt.cabocha\n",
    " ```\n",
    " \n",
    "### memo\n",
    "クラスに`__str__`を定義するとprint等で呼ばれた時の表示方法を指定できる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "        self.jp2en = {\"表層形\": surface, \"基本形\": base, \"品詞\": pos, \"品詞細分類1\": pos1}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.surface\n",
    "    \n",
    "    def check(self, speech, kind):\n",
    "        return self.jp2en[speech] == kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['名前', 'は', 'まだ', '無い', '。']\n"
     ]
    }
   ],
   "source": [
    "prg_morph = re.compile(r\"(?P<sur>.+?)\\t(?P<pos>[^,]+),(?P<pos1>[^,]+),([^,]+,){4}(?P<base>[^,]+).*\")\n",
    "\n",
    "def gen_sentence_morph():\n",
    "    morph_list = []\n",
    "    with open(datapath, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            res_m = prg_morph.match(line)\n",
    "            if res_m:\n",
    "                morph_list.append(Morph(res_m.group(\"sur\"), res_m.group(\"base\"), res_m.group(\"pos\"), res_m.group(\"pos1\")))\n",
    "            elif line == \"EOS\\n\":\n",
    "                yield morph_list\n",
    "                morph_list = []\n",
    "\n",
    "for i, sentence in enumerate(gen_sentence_morph()):\n",
    "    if i== 3:\n",
    "        print(list(map(str,sentence)))\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
    "\n",
    "### memo\n",
    "文節、形態素、文末の判定は正規表現でやる\n",
    "\n",
    "1. 文節\n",
    "\n",
    "    - 形態素は以降の行に書いてあるので後でappendするための受け皿をつくっておく。\n",
    "    - 係り元を覚えておくための辞書を作る。日本語は係り先の方が文の後ろにあるから先読みせず1行ずつ処理可能\n",
    "    - EOSが2行以上連なってるところが1文にカウントされてしまっている\n",
    "    \n",
    "2. 形態素\n",
    "\n",
    "    4章を丸々流用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, index, dst, srcs):\n",
    "        self.morphs = []\n",
    "        self.index = index\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"形態素をくっつけたものを表示(句読点除く)\"\n",
    "        return re.sub('[、。「」]', '', ''.join(map(str, self.morphs)))\n",
    "    \n",
    "    def __int__(self):\n",
    "        \"文中での通し番号を返す\"\n",
    "        return self.index\n",
    "    \n",
    "    def append(self, morph):\n",
    "        \"文節に形態素を追加\"\n",
    "        self.morphs.append(morph)\n",
    "        \n",
    "    def get(self, speech, kind):\n",
    "        \"品詞(speech)が特定の種類(kind)の形態素のみを取り出す\"\n",
    "        return [morph for morph in self.morphs if morph.check(speech, kind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 8\n",
      "この \t-> 書生というのは\n",
      "書生というのは \t-> 話である\n",
      "時々 \t-> 捕えて\n",
      "我々を \t-> 捕えて\n",
      "捕えて \t-> 煮て\n",
      "煮て \t-> 食うという\n",
      "食うという \t-> 話である\n",
      "話である \t-> None\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "prg_morph = re.compile(r\"(?P<sur>.+?)\\t(?P<pos>[^,]+),(?P<pos1>[^,]+),([^,]+,){4}(?P<base>[^,]+).*\")\n",
    "prg_chunk = re.compile(r\"\\* (?P<num>\\d+) (?P<dst>-?\\d+)D \\d+/\\d+ .*\")\n",
    "\n",
    "def gen_sentence():\n",
    "    chunk = None # 名前確保用\n",
    "    sentence, chunk_srcs = [], defaultdict(list)\n",
    "    with open(datapath, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            res_m = prg_morph.match(line)\n",
    "            res_c = prg_chunk.match(line)\n",
    "            if res_c:\n",
    "                me, dst = int(res_c.group(\"num\")), int(res_c.group(\"dst\"))\n",
    "                chunk_srcs[dst].append(me)\n",
    "                chunk = Chunk(me, dst, chunk_srcs[me])\n",
    "                sentence.append(chunk)\n",
    "            elif res_m:\n",
    "                chunk.append(Morph(res_m.group(\"sur\"), res_m.group(\"base\"), res_m.group(\"pos\"), res_m.group(\"pos1\")))\n",
    "            else: # End of sentence\n",
    "                yield sentence\n",
    "                sentence, chunk_srcs = [], defaultdict(list)\n",
    "\n",
    "def dependency_parsing(n):\n",
    "    counter = 0\n",
    "    for sentence in gen_sentence():\n",
    "        if sentence:\n",
    "            counter += 1\n",
    "            if counter == n:\n",
    "                print(\"sentence\",counter)\n",
    "                for chunk in sentence:\n",
    "                    print(chunk, \"\\t->\", sentence[chunk.dst] if chunk.dst != -1 else None)\n",
    "\n",
    "dependency_parsing(8)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/42.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            if chunk.dst == -1:\n",
    "                continue\n",
    "                \n",
    "            line = '\\t'.join(map(str, [chunk, sentence[chunk.dst]]))\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/43.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            if chunk.get(\"品詞\", \"名詞\") and sentence[chunk.dst].get(\"品詞\", \"動詞\"):\n",
    "                line = '\\t'.join(map(str, [chunk, sentence[chunk.dst]]))\n",
    "                f.write(line + \"\\n\") if chunk.dst != -1 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9963 Done 1621 Done DoneDoneDone\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "with PdfPages(\"out/44.pdf\") as pdf:\n",
    "    for sid, sentence in enumerate(gen_sentence()):\n",
    "        node_label = {chunk.index: str(chunk) for chunk in sentence}\n",
    "        edges = [(chunk.index, chunk.dst) for chunk in sentence if chunk.dst != -1]\n",
    "        if node_label:\n",
    "            G = nx.DiGraph()\n",
    "            G.add_nodes_from(node_label.keys())\n",
    "            G.add_edges_from(edges)\n",
    "\n",
    "            # draw graph\n",
    "            plt.clf()\n",
    "            plt.title(\"dependencies in sentence\" + str(sid))\n",
    "            settings = {\n",
    "                'pos': nx.spring_layout(G, k=1.5, pos=nx.spectral_layout(G)),\n",
    "                'node_size': 100,\n",
    "                'node_color': '#afeeee',\n",
    "                'edge_color': '#888888',\n",
    "                'labels': node_label,\n",
    "                'font_family': 'IPAexGothic',\n",
    "                'with_label': True,\n",
    "                'font_size': 5,\n",
    "            }\n",
    "            nx.draw(G, **settings)\n",
    "\n",
    "            # save figure to pdf\n",
    "            pdf.savefig()\n",
    "            \n",
    "        print(\"Page\", sid,\"Done\", end=\"\\r\")\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "### memo\n",
    "1. 動詞入りの文節を探す\n",
    "1. 最左動詞を持ってくる\n",
    "1. 係り元の文節を列挙\n",
    "1. 係り元の文節内の最右助詞を抽出、ソート\n",
    "\n",
    "参考: 助詞[(wikipedia)](https://ja.wikipedia.org/wiki/助詞#格助詞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/45_cp.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            verbs_in_chunk = chunk.get(\"品詞\", \"動詞\")\n",
    "            if verbs_in_chunk:\n",
    "                predicate, *rest = verbs_in_chunk # 最左動詞\n",
    "                chunk_srcs = [sentence[i] for i in chunk.srcs]\n",
    "                particles_in_chunks = [chunk_src.get(\"品詞\", \"助詞\") for chunk_src in chunk_srcs]\n",
    "                particles = [ps[-1].base for ps in particles_in_chunks if ps]\n",
    "                if particles:\n",
    "                    f.write(''.join([predicate.base, '\\t', ' '.join(sorted(particles)), '\\n']))\n",
    "\n",
    "\n",
    "# 動詞→助詞のペアを列挙\n",
    "with open(\"out/45_cp_all.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            verbs_in_chunk = chunk.get(\"品詞\", \"動詞\")\n",
    "            if verbs_in_chunk:\n",
    "                predicate, *rest = verbs_in_chunk # 最左動詞\n",
    "                chunk_srcs = [sentence[i] for i in chunk.srcs]\n",
    "                particles_in_chunks = [chunk_src.get(\"品詞\", \"助詞\") for chunk_src in chunk_srcs]\n",
    "                particles = [ps[-1].base for ps in particles_in_chunks if ps]\n",
    "                for p in particles:\n",
    "                    f.write(''.join([predicate.base, '\\t', p, '\\n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "\n",
    "```sort out/45_cp_all.txt | sort -f | uniq -c | sort -k 1r,1 -k 2f,2 > out/45.count```\n",
    "\n",
    "- 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
    "\n",
    "```grep -E \"^する\" out/45_cp_all.txt | cut -f 2- | sort | uniq -c | sort -rk 1 | awk '{print $2}' | tr '\\n' ' ' > out/45.suru```  \n",
    "```grep -E \"^見る\" out/45_cp_all.txt | cut -f 2- | sort | uniq -c | sort -rk 1 | awk '{print $2}' | tr '\\n' ' ' > out/45.miru```  \n",
    "```grep -E \"^与える\" out/45_cp_all.txt | cut -f 2- | sort | uniq -c | sort -rk 1 | awk '{print $2}' | tr '\\n' ' ' > out/45.ataeru```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "### memo\n",
    "45に加えて...  \n",
    "タプルで持って助詞で辞書順ソートした後に各々を分けて列挙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "with open(\"out/46.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            verbs_in_chunk = chunk.get(\"品詞\", \"動詞\")\n",
    "            if verbs_in_chunk:\n",
    "                predicate, *rest = verbs_in_chunk\n",
    "                chunk_srcs = [sentence[i] for i in chunk.srcs]\n",
    "                particles_in_chunks = [chunk.get(\"品詞\", \"助詞\") for chunk_src in chunk_srcs]\n",
    "                pscs = [(str(ps[-1]), str(cs)) for ps, cs in zip(particles_in_chunks, chunk_srcs) if ps]\n",
    "                pscs.sort(key=lambda t: t[0])\n",
    "                ps, cs =[p for p, c in pscs], [c for p, c in pscs]\n",
    "                if pscs:\n",
    "                    f.write(''.join([predicate.base, '\\t', ' '.join(ps), '\\t', ' '.join(cs), '\\n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "### memo\n",
    "46に加えて...  \n",
    "- 文節が「サ変接続→を」を持つかどうか調べる\n",
    "- 持ってたらそいつの係り先が動詞を持つか探す、あとは46と同様\n",
    "- 出力フォーマットを少し修正する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk2sw = lambda chunk: ''.join([\"s\" if morph.pos1 == \"サ変接続\" else \"w\" if str(morph) == \"を\" else \"x\" for morph in chunk.morphs])\n",
    "chunk2xp = lambda chunk: ''.join([\"p\" if morph.pos == \"助詞\" else \"x\" for morph in chunk.morphs])\n",
    "prg_find_sw = re.compile(\"sw\")\n",
    "prg_last_particle = re.compile(\"p[^p]*?$\")\n",
    "\n",
    "with open(\"out/47.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            # 「サ変->を」の抽出\n",
    "            match_sw = prg_find_sw.search(chunk2sw(chunk))        \n",
    "            if match_sw:\n",
    "                sahen = chunk.morphs[match_sw.start()].surface # サ変名詞を取得\n",
    "                verbs_in_chunk = sentence[chunk.dst].get(\"品詞\",\"動詞\")\n",
    "                if verbs_in_chunk:\n",
    "                    predicate, *rest = verbs_in_chunk # 最左動詞を取得\n",
    "                    chunk_srcs = [sentence[i] for i in sentence[chunk.dst].srcs if not i == chunk.index] # 係り元の列挙\n",
    "                    matches_p = [prg_last_particle.search(chunk2xp(chunk_src)) for chunk_src in chunk_srcs] # 最右助詞の探索\n",
    "                    pscs = [(str(cs.morphs[m.start()]), str(cs)) for m, cs in zip(matches_p, chunk_srcs) if m is not None] # タプルの作成と文字列化\n",
    "                    if pscs:\n",
    "                        pscs.sort(key=lambda t: t[0])\n",
    "                        ps, cs =[p for p, c in pscs], [c for p, c in pscs]\n",
    "                        f.write(''.join([sahen, 'を', predicate.base, '\\t', ' '.join(ps), '\\t', ' '.join(cs), '\\n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/48.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            if chunk.get(\"品詞\", \"名詞\"):\n",
    "                now_chunk, path = chunk, []\n",
    "                while True:\n",
    "                    path.append(str(now_chunk))\n",
    "                    if now_chunk.dst == -1:\n",
    "                        break\n",
    "                    \n",
    "                    now_chunk = sentence[now_chunk.dst]\n",
    "                #print(\" -> \".join(path))\n",
    "                f.write(\" -> \".join(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がi\n",
    "とj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"->\"で連結して表現する\n",
    "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "1. 文節iから構文木の根に至る経路上に文節jが存在する場合\n",
    "\n",
    "文節iから文節jのパスを表示\n",
    "\n",
    "2. 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合  \n",
    "\n",
    "文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を\"|\"で連結して表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def chunk_path(sentence, start_index):\n",
    "    chunk, path = sentence[start_index], []\n",
    "    while True:\n",
    "        path.append(chunk)\n",
    "        if chunk.dst == -1:\n",
    "            break\n",
    "            \n",
    "        chunk = sentence[chunk.dst]\n",
    "        \n",
    "    return path\n",
    "\n",
    "with open(\"out/49.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for n, sentence in enumerate(gen_sentence()):\n",
    "        f.write(\"sentence \"+str(n)+\"\\n\")\n",
    "        \n",
    "        # 名詞句を抽出\n",
    "        indexes = [chunk.index for chunk in sentence if chunk.get(\"品詞\", \"名詞\")]\n",
    "        if len(indexes) < 2:\n",
    "            continue\n",
    "            \n",
    "        for i, j in combinations(indexes, 2):\n",
    "            path_i = chunk_path(sentence, i)\n",
    "            path_i_indexes = list(map(int, path_i))\n",
    "            chunk_i, *rest_i = path_i\n",
    "            subX = ''.join([\"X\" if morph.check(\"品詞\",\"名詞\") else str(morph) for morph in chunk_i.morphs])\n",
    "            \n",
    "            if j in path_i_indexes: # (1)\n",
    "                end = list(map(int, rest_i)).index(j)\n",
    "        \n",
    "                f.write(' -> '.join([subX] + list(map(str,rest_i[:end])) + [\"Y\"]) + \"\\n\")\n",
    "            else: # (2)\n",
    "                path_j = chunk_path(sentence, j)\n",
    "                path_j_indexes = list(map(int, path_j))\n",
    "                chunk_j, *rest_j = path_j\n",
    "                subY = ''.join([\"Y\" if morph.check(\"品詞\",\"名詞\") else str(morph) for morph in chunk_j.morphs])\n",
    "                k = min(set(path_i_indexes) & set(path_j_indexes))\n",
    "                string_k = str(sentence[k])\n",
    "                \n",
    "                end_i = list(map(int, rest_i)).index(k)\n",
    "                string_ik = ' -> '.join([subX] + list(map(str,rest_i[:end_i])))\n",
    "                \n",
    "                end_j = list(map(int, rest_j)).index(k)\n",
    "                string_jk = ' -> '.join([subY] + list(map(str,rest_j[:end_j])))\n",
    "                \n",
    "                f.write(\" | \".join([string_ik, string_jk, string_k]) + \"\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
