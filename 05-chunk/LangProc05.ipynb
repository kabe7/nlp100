{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "datapath = \"data/neko.txt.cabocha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．\n",
    "\n",
    "### 準備\n",
    " ```\n",
    " brew install cabocha\n",
    " cabocha -f1 neko.txt > neko.txt.cabocha\n",
    " ```\n",
    " \n",
    "### memo\n",
    "クラスに`__repr__`を定義するとprint等で呼ばれた時の表示方法を指定できる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['名前', 'は', 'まだ', '無い', '。']\n"
     ]
    }
   ],
   "source": [
    "prg_morph = re.compile(r\"(?P<sur>.+?)\\t(?P<pos>[^,]+),(?P<pos1>[^,]+),([^,]+,){4}(?P<base>[^,]+).*\")\n",
    "\n",
    "def gen_sentence():\n",
    "    morph_list = []\n",
    "    with open(datapath) as f:\n",
    "        for line in f:\n",
    "            res_m = prg_morph.match(line)\n",
    "            if res_m:\n",
    "                morph_list.append(Morph(res_m.group(\"sur\"), res_m.group(\"base\"), res_m.group(\"pos\"), res_m.group(\"pos1\")))\n",
    "            elif line == \"EOS\\n\":\n",
    "                yield morph_list\n",
    "                morph_list = []\n",
    "\n",
    "for i, sentence in enumerate(gen_sentence()):\n",
    "    if i== 3:\n",
    "        print(list(map(str,sentence)))\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．\n",
    "\n",
    "### memo\n",
    "文節、形態素、文末の判定は正規表現でやる\n",
    "\n",
    "1. 文節\n",
    "\n",
    "    - 形態素は以降の行に書いてあるので後でappendするための受け皿をつくっておく。\n",
    "    - 係り元を覚えておくための辞書を作る。日本語は係り先の方が文の後ろにあるから先読みせず1行ずつ処理可能\n",
    "    - EOSが2行以上連なってるところが1文にカウントされてしまっている\n",
    "    \n",
    "2. 形態素\n",
    "\n",
    "    4章を丸々流用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, dst, srcs):\n",
    "        self.morphs = []\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"形態素をくっつけたものを表示(句読点除く)\"\n",
    "        return re.sub('[、。「」]', '', ''.join(map(str, self.morphs)))\n",
    "    \n",
    "    def append(self, morph):\n",
    "        \"文節に形態素を追加\"\n",
    "        self.morphs.append(morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 8\n",
      "この \t-> 書生というのは\n",
      "書生というのは \t-> 話である。\n",
      "時々 \t-> 捕えて\n",
      "我々を \t-> 捕えて\n",
      "捕えて \t-> 煮て\n",
      "煮て \t-> 食うという\n",
      "食うという \t-> 話である。\n",
      "話である。 \t-> None\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "prg_morph = re.compile(r\"(?P<sur>.+?)\\t(?P<pos>[^,]+),(?P<pos1>[^,]+),([^,]+,){4}(?P<base>[^,]+).*\")\n",
    "prg_chunk = re.compile(r\"\\* (?P<num>\\d+) (?P<dst>-?\\d+)D \\d+/\\d+ .*\")\n",
    "\n",
    "def gen_sentence():\n",
    "    chunk = Chunk(-1, []) # 名前確保用\n",
    "    sentence, chunk_srcs = [], defaultdict(list)\n",
    "    with open(datapath) as f:\n",
    "        for line in f:\n",
    "            res_m = prg_morph.match(line)\n",
    "            res_c = prg_chunk.match(line)\n",
    "            if res_c:\n",
    "                me, dst = int(res_c.group(\"num\")), int(res_c.group(\"dst\"))\n",
    "                chunk_srcs[dst].append(me)\n",
    "                chunk = Chunk(dst, chunk_srcs[me])\n",
    "                sentence.append(chunk)\n",
    "            elif res_m:\n",
    "                chunk.append(Morph(res_m.group(\"sur\"), res_m.group(\"base\"), res_m.group(\"pos\"), res_m.group(\"pos1\")))\n",
    "            else: # End of sentence\n",
    "                yield sentence\n",
    "                sentence, chunk_srcs = [], defaultdict(list)\n",
    "\n",
    "def dependency_parsing(n):\n",
    "    counter = 0\n",
    "    for sentence in gen_sentence():\n",
    "        if sentence:\n",
    "            counter += 1\n",
    "            if counter == n:\n",
    "                print(\"sentence\",counter)\n",
    "                for chunk in sentence:\n",
    "                    print(chunk, \"\\t->\", sentence[chunk.dst] if chunk.dst != -1 else None)\n",
    "\n",
    "dependency_parsing(8)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/43.txt\", \"w\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            if \"名詞\" in [m.pos for m in chunk.morphs] and \"動詞\" in [m.pos for m in sentence[chunk.dst].morphs]:\n",
    "                line = '\\t'.join(map(str, [chunk, sentence[chunk.dst]]))\n",
    "                f.write(line + \"\\n\") if chunk.dst != -1 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "### memo\n",
    "1. 動詞入りの文節を探す\n",
    "1. 最左動詞を持ってくる\n",
    "1. 係り元の文節を列挙\n",
    "1. 係り元の文節内の助詞を抽出、ソート\n",
    "\n",
    "参考: 助詞[(wikipedia)](https://ja.wikipedia.org/wiki/助詞#格助詞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "with open(\"out/45_cp.txt\", \"w\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            morph_verbs = [morph.base for morph in chunk.morphs if morph.pos == \"動詞\"]\n",
    "            if morph_verbs:\n",
    "                predicate, *rest = morph_verbs # 最左動詞\n",
    "                morph_srcs = chain.from_iterable(sentence[i].morphs for i in chunk.srcs) # list(list(morph))のflatten\n",
    "                particles = [morph.base for morph in morph_srcs if morph.pos == \"助詞\"] # 助詞の抽出\n",
    "                if particles:\n",
    "                    f.write(''.join([predicate, '\\t', ' '.join(sorted(particles)), '\\n']))\n",
    "\n",
    "\n",
    "# 動詞→助詞のペアを列挙\n",
    "with open(\"out/45_cp_all.txt\", \"w\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            morph_verbs = [morph.base for morph in chunk.morphs if morph.pos == \"動詞\"]\n",
    "            if morph_verbs:\n",
    "                predicate, *rest = morph_verbs\n",
    "                morph_srcs = chain.from_iterable(sentence[i].morphs for i in chunk.srcs)\n",
    "                particles = [morph.base for morph in morph_srcs if morph.pos1 == \"格助詞\"]\n",
    "                for p in particles:\n",
    "                    f.write(''.join([predicate, '\\t', p, '\\n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "\n",
    "```sort out/45_cp_all.txt | sort -f | uniq -c | sort -k 1r,1 -k 2f,2 > out/45_cp_pairs.count```\n",
    "\n",
    "- 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
    "\n",
    "```grep -E \"^する\" out/45_cp_all.txt | cut -f 2- | sort | uniq -c | sort -rk 1 | awk '{print $2}' | tr '\\n' ' ' > out/45_suru.txt```  \n",
    "```grep -E \"^見る\" out/45_cp_all.txt | cut -f 2- | sort | uniq -c | sort -rk 1 | awk '{print $2}' | tr '\\n' ' ' > out/45_miru.txt```  \n",
    "```grep -E \"^与える\" out/45_cp_all.txt | cut -f 2- | sort | uniq -c | sort -rk 1 | awk '{print $2}' | tr '\\n' ' ' > out/45_ataeru.txt```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "### memo\n",
    "45に加えて...  \n",
    "タプルで持って助詞で辞書順ソートした後に各々を分けて列挙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "with open(\"out/46.txt\", \"w\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            morph_verbs = [morph.base for morph in chunk.morphs if morph.pos == \"動詞\"]\n",
    "            if morph_verbs:\n",
    "                predicate, *rest = morph_verbs\n",
    "                pscs = [(morph.base, str(sentence[i])) for i in chunk.srcs for morph in sentence[i].morphs if morph.pos == \"助詞\"]\n",
    "                pscs.sort(key=lambda t: t[0])\n",
    "                ps, cs =[p for p, c in pscs], [c for p, c in pscs]\n",
    "                if pscs:\n",
    "                    f.write(''.join([predicate, '\\t', ' '.join(ps), '\\t', ' '.join(cs), '\\n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "### memo\n",
    "46に加えて...  \n",
    "- 文節が「サ変接続→を」を持つかどうか調べる\n",
    "- 持ってたらそいつの係り先が動詞を持つか探す、あとは46と同様\n",
    "- 出力フォーマットを少し修正する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "with open(\"out/47.txt\", \"w\") as f:\n",
    "    for sentence in gen_sentence():\n",
    "        for chunk in sentence:\n",
    "            #　サ変->を　の抽出\n",
    "            find_sw = ''.join([\"s\" if morph.pos1 == \"サ変接続\" else \"w\" if str(morph) == \"を\" else \"x\" for morph in chunk.morphs])\n",
    "            match = re.search(\"sw\", find_sw)        \n",
    "            if match:\n",
    "                sahen = chunk.morphs[match.start()].surface\n",
    "                morph_verbs = [morph.base for morph in sentence[chunk.dst].morphs if morph.pos == \"動詞\"]\n",
    "                if morph_verbs:\n",
    "                    predicate, *rest = morph_verbs\n",
    "                    pscs = [(morph.base, str(sentence[i])) for i in sentence[chunk.dst].srcs for morph in sentence[i].morphs if morph.pos == \"助詞\"]\n",
    "                    pscs.sort(key=lambda t: t[0])\n",
    "                    ps, cs =[p for p, c in pscs], [c for p, c in pscs]\n",
    "                    if pscs:\n",
    "                        f.write(''.join([sahen, 'を', predicate, '\\t', ' '.join(ps), '\\t', ' '.join(cs), '\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
